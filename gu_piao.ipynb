{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":101369,"databundleVersionId":12211017,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 导入必要的库\nimport pandas as pd              # 数据处理，核心函数：pd.read_csv(), DataFrame操作\nimport numpy as np               # 数值计算，支持多维数组和数学运算\nfrom sklearn.model_selection import (\n    train_test_split,            # 数据集划分函数，参数：test_size, random_state\n    cross_val_score              # 交叉验证得分计算（本代码未直接使用）\n)\nfrom sklearn.linear_model import (\n    LinearRegression,            # 线性回归模型（最小二乘法）\n    Ridge,                       # 岭回归（L2正则化），参数：alpha（正则化强度）\n    Lasso                        # Lasso回归（L1正则化），参数：alpha（正则化强度）\n)\nfrom sklearn.ensemble import RandomForestRegressor  # 随机森林回归，参数：n_estimators（树的数量）\nfrom sklearn.preprocessing import StandardScaler    # 标准化（均值0，标准差1）\nfrom sklearn.metrics import (\n    mean_squared_error,          # 均方误差（MSE），越小越好\n    r2_score                     # R²分数，越接近1越好\n)\nimport matplotlib.pyplot as plt  # 绘图库，用于可视化结果\n\n# 可选导入XGBoost和LightGBM（如果未安装则跳过）\ntry:\n    from xgboost import XGBRegressor  # XGBoost回归模型，参数：n_estimators, verbosity（日志输出）\n    xgb_installed = True\nexcept ImportError:\n    xgb_installed = False        # 标记XGBoost是否可用\ntry:\n    from lightgbm import LGBMRegressor  # LightGBM回归模型，参数：n_estimators\n    lgb_installed = True\nexcept ImportError:\n    lgb_installed = False        # 标记LightGBM是否可用\n\n# --------------------------\n# 工具函数定义\n# --------------------------\ndef ensure_id_column(df, start_id=1501):\n    \"\"\"\n    确保数据框包含'id'列，若无则自动生成从start_id开始的连续ID\n    参数:\n        df (pd.DataFrame): 输入数据框\n        start_id (int): 起始ID值，默认为1501\n    返回:\n        pd.DataFrame: 包含'id'列的数据框\n    \"\"\"\n    if 'id' not in df.columns:\n        df['id'] = range(start_id, start_id + len(df))  # 生成连续ID\n    return df\n\ndef feature_engineering(df, is_train=True):\n    \"\"\"\n    特征工程函数，处理日期字段并生成统计特征\n    参数:\n        df (pd.DataFrame): 输入数据框\n        is_train (bool): 是否为训练集（True则生成标签，False则跳过）\n    返回:\n        X (pd.DataFrame): 特征矩阵\n        y (pd.Series): 标签列（仅当is_train=True时返回）\n    \"\"\"\n    df = df.copy()\n    # 日期字段处理（如果存在'date'列）\n    if 'date' in df.columns:\n        df['date'] = pd.to_datetime(df['date'])  # 转为datetime类型\n        df['year'] = df['date'].dt.year          # 提取年份\n        df['month'] = df['date'].dt.month        # 提取月份\n        df['day'] = df['date'].dt.day            # 提取日\n        df['day_of_week'] = df['date'].dt.dayofweek  # 提取星期几（0=周一，6=周日）\n        df = df.drop(columns='date')             # 删除原始日期列\n    \n    # 统计特征生成（仅训练集计算滚动特征，测试集填0保持特征一致性）\n    if is_train and 'close' in df.columns:\n        # 3日收盘价滚动均值和标准差（min_periods=1允许窗口不满时计算）\n        df['close_rolling_mean3'] = df['close'].rolling(window=3, min_periods=1).mean()\n        df['close_rolling_std3'] = df['close'].rolling(window=3, min_periods=1).std().fillna(0)\n    else:\n        # 测试集填充同名特征（避免特征维度不一致）\n        df['close_rolling_mean3'] = 0\n        df['close_rolling_std3'] = 0\n    \n    # 分离特征和标签\n    if is_train:\n        X = df.drop(columns=['id', 'close'], errors='ignore')  # 删除ID和标签列（errors='ignore'防止列不存在时报错）\n        y = df['close']\n    else:\n        X = df.drop(columns=['id'], errors='ignore')\n        y = None\n    return X, y\n\n# --------------------------\n# 主函数\n# --------------------------\ndef main():\n    # 1. 数据读取\n    train = pd.read_csv('/kaggle/input/houkongtest/stock_train_data.csv')  # 读取训练集\n    test = pd.read_csv('/kaggle/input/houkongtest/stock_test_data.csv')    # 读取测试集\n    test = ensure_id_column(test, start_id=1501)  # 确保测试集有'id'列\n\n    # 2. 特征工程\n    X, y = feature_engineering(train, is_train=True)   # 处理训练集，生成特征和标签\n    X_test, _ = feature_engineering(test, is_train=False)  # 处理测试集，仅生成特征\n    id_test = test['id']  # 保存测试集ID用于最终结果\n\n    # 3. 划分验证集\n    # train_test_split参数：\n    # - test_size=0.2: 验证集占比20%\n    # - random_state=42: 固定随机种子保证可复现\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # 4. 标准化处理\n    scaler = StandardScaler()               # 初始化标准化器\n    X_tr_scaled = scaler.fit_transform(X_tr)  # 在训练集上拟合标准化参数并转换\n    X_val_scaled = scaler.transform(X_val)  # 验证集转换（使用训练集的参数）\n    X_test_scaled = scaler.transform(X_test)  # 测试集转换\n\n    # 5. 多模型对比\n    models = [\n        ('LinearRegression', LinearRegression()),  # 线性回归（无正则化）\n        ('Ridge', Ridge(alpha=1.0)),                # 岭回归（alpha=1.0）\n        ('Lasso', Lasso(alpha=0.1)),                # Lasso回归（alpha=0.1）\n        ('RandomForest', RandomForestRegressor(n_estimators=200, random_state=42)),  # 随机森林（200棵树）\n    ]\n    # 如果安装了XGBoost和LightGBM则添加到模型列表\n    if xgb_installed:\n        models.append(('XGBoost', XGBRegressor(n_estimators=200, random_state=42, verbosity=0)))  # verbosity=0关闭训练日志\n    if lgb_installed:\n        models.append(('LightGBM', LGBMRegressor(n_estimators=200, random_state=42)))\n\n    # 模型训练与评估\n    best_score = -np.inf  # 初始化最佳R²为负无穷\n    best_model = None     # 保存最优模型对象\n    best_name = \"\"        # 保存最优模型名称\n    results = []          # 保存所有模型结果\n\n    for name, model in models:\n        model.fit(X_tr_scaled, y_tr)                   # 训练模型\n        val_pred = model.predict(X_val_scaled)         # 预测验证集\n        r2 = r2_score(y_val, val_pred)                # 计算R²\n        mse = mean_squared_error(y_val, val_pred)      # 计算MSE\n        results.append((name, r2, mse))                # 记录结果\n        print(f\"{name} 验证集R2: {r2:.4f}, MSE: {mse:.4f}\")  # 打印结果\n        if r2 > best_score:                            # 更新最优模型\n            best_score = r2\n            best_model = model\n            best_name = name\n\n    print(f\"\\n最优模型：{best_name}，验证集R2: {best_score:.4f}\")  # 输出最优模型\n\n    # 6. 可视化最优模型效果\n    val_pred = best_model.predict(X_val_scaled)        # 最优模型预测验证集\n    plt.figure(figsize=(8,6))                          # 设置画布大小\n    plt.scatter(y_val, val_pred, alpha=0.6)            # 绘制真实值vs预测值散点图\n    plt.xlabel('真实值')                                # x轴标签\n    plt.ylabel('预测值')                                # y轴标签\n    plt.title(f'最优模型({best_name})验证集预测效果')     # 标题\n    plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--')  # 绘制红色对角线（理想情况）\n    plt.show()                                          # 显示图表\n\n    # 7. 预测测试集并保存结果\n    test_pred = best_model.predict(X_test_scaled)       # 预测测试集\n    res_df = pd.DataFrame({'id': id_test, 'close': test_pred})  # 创建结果数据框\n    res_df.to_csv('/kaggle/working/ccf.csv', index=False)       # 保存为CSV（不保留索引）\n    print(\"最优模型预测结果已保存至 /kaggle/working/ccf.csv\")    # 输出提示\n\n# 程序入口\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}
