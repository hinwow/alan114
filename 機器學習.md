以下是包含原理解釋、詳細用法和代碼示例的 **Scikit-learn 機器學習方法總結**：

---

## **1. 監督學習**

監督學習是通過帶標籤的數據進行訓練，學習輸入與輸出之間的映射，常見於分類（離散輸出）和回歸（連續輸出）問題。

---

### **1.1 線性回歸 (Linear Regression)**

#### **原理**
- 設定數據集為：`X`（特徵），`y`（目標值）。
- 假設輸入和輸出之間的關係為線性：  
  **y = wX + b**，其中 `w` 是權重，`b` 是偏置。
- 目標是最小化損失函數（均方誤差，MSE）：  
  **MSE = (1/n) Σ (y_true - y_pred)^2**

#### **用法**
- 適用於數據分佈線性相關的回歸問題。
- 支援多特徵輸入和多輸出。

#### **代碼示例**

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 構造數據集
import numpy as np
X = np.array([[1], [2], [3], [4], [5]])  # 特徵
y = np.array([1.5, 3.3, 5.1, 6.7, 9.0])  # 目標值

# 拆分數據集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 創建線性回歸模型
model = LinearRegression()
model.fit(X_train, y_train)  # 訓練模型

# 預測
y_pred = model.predict(X_test)

# 評估模型
print("權重 (w):", model.coef_)
print("偏置 (b):", model.intercept_)
print("均方誤差 (MSE):", mean_squared_error(y_test, y_pred))
```

---

### **1.2 Logistic 回歸 (Logistic Regression)**

#### **原理**
- 用於分類，通過 **Sigmoid 函數** 將線性輸出映射到 [0, 1]：
  **σ(z) = 1 / (1 + e^(-z))**，其中 **z = wX + b**。
- 結果可解釋為某類的概率，閾值通常設為 0.5。
- 損失函數為對數損失（log-loss）。

#### **用法**
- 適用於二分類、多分類問題。
- 可設置正則化參數（`C`）防止過擬合。

#### **代碼示例**

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# 構造數據集
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([0, 0, 1, 1, 1])  # 二分類標籤

# 拆分數據集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 創建 Logistic 回歸模型
model = LogisticRegression()
model.fit(X_train, y_train)

# 預測
y_pred = model.predict(X_test)

# 評估模型
print("準確率 (Accuracy):", accuracy_score(y_test, y_pred))
print("分類報告:\n", classification_report(y_test, y_pred))
```

---

### **1.3 K 最近鄰 (K-Nearest Neighbors, KNN)**

#### **原理**
- 基於「距離最近」的思想，將輸入樣本分配到最近的 `K` 個鄰居中出現最多的類別。
- 常用距離度量：**歐幾里得距離**。

#### **用法**
- 適用於分類和回歸。
- 需要調整超參數 `K`，較小的 `K` 容易過擬合，較大的 `K` 容易欠擬合。

#### **代碼示例**

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 構造數據集
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])
y = np.array([0, 0, 1, 1, 1])  # 類別標籤

# 創建 KNN 模型
model = KNeighborsClassifier(n_neighbors=3)  # 設置 K=3
model.fit(X, y)

# 預測
X_test = np.array([[2.5, 3.5]])
y_pred = model.predict(X_test)

print("預測類別:", y_pred)
```

---

### **1.4 決策樹 (Decision Tree)**

#### **原理**
- 以樹形結構進行數據劃分，使用 **信息增益** 或 **基尼不純度** 來選擇最佳劃分節點。
- 遞歸構建分支，直至滿足停止條件（如樹深度）。

#### **用法**
- 適用於分類和回歸。
- 需要防止過擬合（可調整樹深、最小樣本數等參數）。

#### **代碼示例**

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# 構造數據集
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])
y = np.array([0, 0, 1, 1, 1])  # 類別標籤

# 創建決策樹模型
model = DecisionTreeClassifier(max_depth=3)  # 限制最大樹深
model.fit(X, y)

# 預測
X_test = np.array([[2.5, 3.5]])
y_pred = model.predict(X_test)

print("預測類別:", y_pred)
```

---

### **1.5 隨機森林 (Random Forest)**

#### **原理**
- 通過集成多棵決策樹進行分類或回歸，採用隨機數據和特徵進行訓練。
- 最終結果通過投票（分類）或平均（回歸）得到。

#### **用法**
- 適用於高維數據，對噪聲和過擬合不敏感。
- 需要調整樹數量（`n_estimators`）和最大深度。

#### **代碼示例**

```python
from sklearn.ensemble import RandomForestClassifier

# 構造數據集
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])
y = np.array([0, 0, 1, 1, 1])  # 類別標籤

# 創建隨機森林模型
model = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=42)
model.fit(X, y)

# 預測
X_test = np.array([[2.5, 3.5]])
y_pred = model.predict(X_test)

print("預測類別:", y_pred)
```

---

## **2. 模型評估與性能衡量**

### **常用指標**
- **Accuracy（準確率）**: 總體正確率。
- **Precision（精確率）**: 預測為正例的樣本中，實際為正例的比例。
- **Recall（召回率）**: 實際正例樣本中，被正確預測為正例的比例。
- **F1-Score**: 精確率和召回率的調和平均。

#### **代碼示例**

```python
from sklearn.metrics import classification_report

# 訓練和測試結果
y_true = [0, 1, 0, 1, 1]
y_pred = [0, 1, 0, 0, 1]

# 評估
print(classification_report(y_true, y_pred))
```

---

這是 **Scikit-learn** 常用監督學習方法的原理、用法和代碼實現，其他算法如非監督學習（如 K-Means、PCA）、超參數調整和交叉驗證，也可以進一步闡述。若需更詳細的案例，請告知！
